<!DOCTYPE html>
<html lang="en">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
		<title>MA2823: Introduction to Machine Learning</title>
		<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link href="./static/css/bootstrap.min.css" rel="stylesheet">
		<link href="./static/css/style.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
	</head>

	<body data-pinterest-extension-installed="cr1.40">
		<a name="home"></a>
		<nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
		  	<div class="container">
			    <!-- Brand and toggle get grouped for better mobile display -->
			    <div class="navbar-header">
					<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				        <span class="sr-only">Toggle navigation</span>
				        <span class="icon-bar"></span>
				        <span class="icon-bar"></span>
				        <span class="icon-bar"></span>
			      	</button>
			      	<a class="navbar-brand" href="#home"><strong>MA2823 - Fall 2018</strong></a>
			    </div>

			    <!-- Collect the nav links, forms, and other content for toggling -->
			    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			      	<ul class="nav navbar-nav navbar-right">
				        <li><a href="#overview"><span class="fa fa-bookmark-o"></span> Overview</a></li>
						<li><a href="#lectures"><span class="fa fa-calendar-check-o"></span> Schedule and Lectures</a></li>
						<li><a href="#structure"><span class="fa fa-file-text-o"></span>  Course Structure</a></li>
						<li><a href="#project"><span class="fa fa-cogs"></span> Project</a></li>
						<li><a href="#resources"><span class="fa fa-external-link"></span> Resources</a></li>
			      	</ul>
			    </div><!-- /.navbar-collapse -->
		  </div><!-- /.container-fluid -->
		</nav>


		<div class="navblank">
		</div>

		<div class="title_box jumbotron">
			<div class="container">
				<h1>MA2823: Introduction to Machine Learning</h1>			
				<div class="description"></div>
			</div>
		</div>

		<div class="container">

			<div class="content_box">
				<a name="overview"></a>
				<h2><span class="fa fa-bookmark-o"></span> Course Overview </h2>
				<blockquote class="text-justify"><p> 
				<b>General Info:</b> 2nd year course, CentraleSup&#233lec, Fall 2018  </br></br>
				<b>Lecture hours:</b> Friday, 13:45 - 17:00 </br>
				<b>Instructor:</b> <a href="http://fragkiskos.me">Fragkiskos Malliaros</a> </br>
				<b>Email:</b> fragkiskos.me [at] gmail.com </br>
				<b>Office hours:</b> Right after class (or send me an email and we will find a good time to meet)</br> </br>

				<b>TAs:</b> Dr. Maria Vakalopoulou (maria.vakalopoulou [at] centralesupelec.fr),  Enzo Battistella (enzo.battistella [at] centralesupelec.fr), Mihir Sahasrabudhe (mihir.sahasrabudhe [at] centralesupelec.fr) </br>
				</br></br>

				<b>Piazza:</b> <a href="https://piazza.com/centralesupelec/fall2018/ma2823/home">piazza.com/centralesupelec/fall2018/ma2823/home</a> </br>

				</p>
				</blockquote>
				<br>
				
				<p align="justify">Machine learning  is the scientific field that provides computers the ability to learn without being explicitly programmed (definition by <a href="https://en.wikipedia.org/wiki/Machine_learning">Wikipedia</a>).  Machine learning lies at the heart of many real-world applications, including recommender systems, web search, computer vision, autonomous cars and automatic language translation.  </p>

				The course will provide an overview of fundamental topics as well as important trends in machine learning, including algorithms for supervised and unsupervised learning, dimensionality reduction methods and their applications. A substantial lab section will involve group projects on a data science competition and will provide the students the ability to apply the course theory to real-world problems. </p>
				<br>
				
				


				<br><br><br>
				<a name="lectures"></a>
				<h2><span class="fa fa-calendar-check-o"></span> Schedule and Lectures </h2>
				The topics of the lectures are subject to change (the following schedule outlines the topics that will be covered in the course). The slides for each lecture will be posted in <a href="piazza.com/centralesupelec/fall2018/ma2823/home">piazza</a> just before the start of the class. <b>The due dates of the assignments/project are subject to change.</b></br></br> 


					<div class="table_scrollable">
					<table class="table" width=100%>
					<col width="5%" />
					<col width="12%" />
					<col width="44%" />
					<col width="10%" />
					<col width="23%" />
						<thead>
							<tr>
								<th class="text-success">Week</th>
								<th class="text-success">Date</th>
								<th class="text-success">Topic</th>
								<th class="text-success">Material</th>
								<th class="text-success">Assignments/Project</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>1</td><td>September 21</td><td>Introduction</td><td><a href="#lecture1">Lecture 1</a></td><td></td>
							</tr>
							<tr>
								<td>2</td><td>September 28</td><td>Dimensionality reduction</td><td><a href="#lecture2">Lecture 2</a></td><td></td> </br>
							</tr>
							<tr>	
								<td>3</td><td>October 5</td><td>Model selection and evaluation</td><td><a href="#lecture3">Lecture 3</a></td><td><b>Assignment 1 out</b></td>
							</tr>
							<tr>	
								<td>4</td><td>October 12</td><td>Linear and logistic regression</td><td><a href="#lecture4">Lecture 4</a></td><td><b> Project proposal due on October 14</b></td>
							</tr>

							<tr>	
								<td>5</td><td>October 19</td><td>Probabilistic classifiers and linear discriminant analysis</td><td><a href="#lecture5">Lecture 5</a></td><td><b>Assignment 2 out</b> </br><b>Assignment 1 due on October 21</b></td>
							</tr>
							<tr>	
								<td>6</td><td>November 9</td><td>Non-parametric learning and nearest neighbor methods</td><td><a href="#lecture6">Lecture 6</a></td><td></td>
							</tr>
							<tr>	
								<td>7</td><td>November 16</td><td>Tree-based methods and ensemble learning</td><td><a href="#lecture7">Lecture 7</a></td><td><b></b></td>
							</tr>
							<tr>	
								<td>8</td><td>November 23</td><td>Support Vector Machines</td><td><a href="#lecture8">Lecture 8</a></td><td></td>
							</tr>
							<tr>	
								<td>9</td><td>November 30</td><td>Introduction to reinforcement learning </br> <font size="2">Guest lecture by <a href="https://ntziortziotis.github.io/">Dr. Nikolaos Tziortziotis</a></font></td><td><a href="#lecture9">Lecture 9</a></td><td><b>Assignment 2 due on December 2</b></td>
							</tr>
							<tr>	
								<td>10</td><td>December 7</td><td>Neural Networks</td><td></td><td></td>
							</tr>
							<tr>	
								<td>11</td><td>December 14</td><td>Unsupervised learning: clustering</td><td></td><td></td>
							</tr>
							<tr>	
								<td>12</td><td>January 11</td><td>Exams</td><td></td><td><b>Project final report due on January 6</b></td>
							</tr>	
						</tbody>
					</table>

				<!--
				<span id="emp"></span> <a href="http://fragkiskosm.github.io/papers/Tutorial_Slides_ICDM_2016.pdf"> Tutorial_Slides</a><br> <br> <br> --><br>
				
				</br>


				</br>
				<a name="lecture1"></a>
				<h2><span class="fa  fa-files-o"></span> [September 21] Lecture 1: Introduction</h2>
				Introduction to machine learning, administrivia, course structure and overview of the topics that will be covered in the course. Bacic concepts in optimization. </br></br>

				Reading: 
				<ul>
					<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapters 1 and 2)
					<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn">The Elements of Statistical Learning</a> (Chapter 1)
				</ul>
				</br>

				Additional:
				<ul>
					<li> <a href="http://sbubeck.com/Bubeck15.pdf">Convex Optimization: Algorithms and Complexity</a> (Sections 1.1, 1.2, 1.3)
					<li> <a href="http://theory.epfl.ch/vishnoi/Nisheeth-VishnoiFall2014-ConvexOptimization.pdf">Convex optimization and gradient descent</a>, lecture notes by Nisheeth Vishnoi, EPFL (Sections 1.1, 1.2, 1.3)
				</ul></br>


				<a name="lecture2"></a>
				<h2><span class="fa  fa-files-o"></span> [September 28] Lecture 2: Dimensionality reduction</h2>
				Dimemensionality reduction techniques. Singular Value Decomposition (SVD). Principal Component Analysis (PCA). Multidimensional Scaling (MDS) and nonlinear dimensionality reduction.
				</br></br>

				Reading: 
				<ul>
					<li> <a href="http://infolab.stanford.edu/~ullman/mmds/ch11.pdf">Mining of Massive Datasets</a> (Sections 11.1, 11.2, 11.3)
					<li> Jonathon Shlens. <a href="https://arxiv.org/pdf/1404.1100.pdf">A Tutorial on Principal Component Analysis</a>. arXiv, 2014.
				</ul></br>

				Additional:
				<ul>
					<li> <a href="http://theory.stanford.edu/~tim/s15/l/l9.pdf"> SVD and Low Rank Matrix Approximations</a>, lecture notes by Tim Roughgarden and Gregory Valiant (Stanford University)
					<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf"> Understanding Machine Learning: From Theory to Algorithms</a> (Section 23.1)
					<li>J. B. Tenenbaum, V. De Silva, and J. C. Langford. <a href="http://wearables.cc.gatech.edu/paper_of_week/isomap.pdf">A Global Geometric Framework for Nonlinear Dimensionality Reduction</a>. Science, 290:5500, pp. 2319-2323, 2000
					<li> S. T. Roweis and L. K. Saul. <a href="http://www.robots.ox.ac.uk/~az/lectures/ml/lle.pdf">Nonlinear Dimensionality Reduction by Locally Linear Embedding</a>. Science, 290:5500, pp. 2323-2326, 2000
					<li> M. Belkin and P. Niyogi. <a href="https://www.cise.ufl.edu/class/cap6617fa15/Readings/LEM_NIPS_01.pdf">Laplacian eigenmaps and spectral techniques for embedding and clustering</a>. In NIPS, 2001	
				</ul></br>


				<a name="lecture3"></a>
				<h2><span class="fa  fa-files-o"></span> [October 5] Lecture 3: Model selection and evaluation</h2>
				Overfitting and generalization. Bias-variance tradeoff. Training, validation and test sets. Cross-validation. Evalution of supervised learning algorithms.
				</br></br>

				Reading: 
				<ul>
				<li> <a href=" https://web.stanford.edu/~hastie/ElemStatLearn">The Elements of Statistical Learning: Data Mining, Inference, and Prediction </a> (Sections 7.1, 7.2, 7.3, 7.10)
				</ul></br>

				Additional:
				<ul>
				<li> M. Kuhn and K. Johnson. <a href="https://link.springer.com/chapter/10.1007/978-1-4614-6849-3_19/fulltext.html">An Introduction to Feature Selection</a>. Applied Predictive Modeling, pages 487-519, 2013. <i>[For the first part of the lecture on feature selection].</i>
				<li> Model evaluation, model selection, and algorithm selection in machine learning: <a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html">Part I</a> (Basics), <a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html">Part II</a> (Bootstrapping and uncertainties), and <a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html">Part III</a> (Cross-validation and hyperparameter tuning). Interesting blog post by <a href="https://sebastianraschka.com/">Sebastian Raschka</a>, 2016.
				</ul></br>



				<a name="lecture4"></a>
				<h2><span class="fa  fa-files-o"></span>  [October 12] Lecture 4: Linear and logistic regression </h2>
				Supervised learning models. Linear regression. Linear classification models. Logistic regression. Maximum likelihood estimation. </br></br>

				Reading: 
				<ul>
 				<li> <a href="https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf">Linear and logistic regression</a>, lecture notes by Andrew Ng (Stanford University)
 				<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn">The Elements of Statistical Learning</a> (Sections 3.1, 3.2 and 3.4)
				</ul>

				Additional:
				<ul>
				<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Sections 9.2 and 9.3)
				</ul></br>

				
				<a name="lecture5"></a>
				<h2><span class="fa  fa-files-o"></span>  [October 19] Lecture 5: Probabilistic classifiers and linear discriminant analysis </h2>
				Bayes rule. Naive Bayes classifier. Maximum a posteriori estimation. Linear discriminant analysis (LDA). </br></br>

				Reading:
				<ul> 
				<li> <a href="https://see.stanford.edu/materials/aimlcs229/cs229-notes2.pdf">Generative learning algorithms</a>, lecture notes by Andrew Ng (Stanford University)
				<li> <a href="https://pdfs.semanticscholar.org/1ab8/ea71fbef3b55b69e142897fadf43b3269463.pdf">Fisher linear discriminant analysis (LDA)</a>, lecture notes by Cheng Li and Bingyu Wang (Northeastern University)	
				</ul>

				Additional:
				<ul>
				<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Sections 24.1, 24.2 and 24.3)
				<li> <a href="https://www.doc.ic.ac.uk/~dfg/ProbabilisticInference/old_IDAPILecture15.pdf">Linear discriminant analysis (LDA)</a>, lecture notes by Duncan Fyfe Gillies (Imperial College)
				<li> <a href="https://www.ics.uci.edu/~welling/teaching/273ASpring09/Fisher-LDA.pdf">Fisher linear discriminant analysis</a>, notes by Max Welling (University of Amsterdam)	
				</ul></br>



				<a name="lecture6"></a>
				<h2><span class="fa  fa-files-o"></span> [November 9] Lecture 6: Non-parametric learning and nearest neighbor methods </h2>
				Introduction to non-parametric learning methods. Distance and similarity metrics. Nearest neighbor algorithms. </br></br>

				Reading: 
				<ul>
				<li> <a href="http://ciml.info/dl/v0_99/ciml-v0_99-ch03.pdf">	A Course in Machine Learning</a>, by Hal Daumé III (Sections 3.1, 3.2 and 3.3)
				</ul>

				Additional:
				<ul>
				<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapter 19)	
				<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn">The Elements of Statistical Learning</a> (Section 13.3)
				</ul></br>

				


				<a name="lecture7"></a>
				<h2><span class="fa  fa-files-o"></span> [November 16] Lecture 7: Tree-based methods and ensemble learning</h2>
				Decision trees. Ensemble learning. Bagging and Boosting. The AdaBoost algorithm.</br></br>

				Reading:
				<ul> 
				<li> <a href="https://www-users.cs.umn.edu/~kumar001/dmbook/ch4.pdf">Classification: Basic Concepts, Decision Trees, and Model Evaluation</a>, Introduction to Data Mining, by Pang-Ning Tan, Michael Steinbach, and Vipin Kumar, 2006
				<li> <a href="https://www.lri.fr/~kegl/mlcourse/book.pdf">Introduction to AdaBoost</a>, lecture notes by Balazs Kegl (University of Paris-Saclay)
				</ul>

				Additional:
				<ul>
				<li><a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapters 18 and 10) 
				</ul></br>


				<a name="lecture8"></a>
				<h2><span class="fa  fa-files-o"></span> [November 23] Lecture 8: Support Vector Machines</h2>
				Maximum margin classifier. Linear SVMs. Primal and dual optimization problems. Non-linearly separable data and the kernel trick. Regularization and the non-separable case. </br></br>
				

				Reading: 
				<ul>
					<li> <a href="https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf">Support Vector Machines</a>, lecture notes by Andrew Ng (Stanford University)
				</ul> </br>

				Additional:
				<ul>
				<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapters 15)
				</ul></br>

				<a name="lecture9"></a>
				<h2><span class="fa  fa-files-o"></span>  [November 30] Lecture 9: Introduction to reinforcement learning </h2>
				Guest lecture by <a href="https://ntziortziotis.github.io/">Dr. Nikolaos Tziortziotis</a>.
				 </br></br>

				Reading: 
				<ul>
					<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapter 20)
				</ul></br>
				
				Additional:
				<ul>
				<li> 
				</ul></br>


				<a name="lecture10"></a>
				<h2><span class="fa  fa-files-o"></span>  Lecture XX: Neural Networks </h2>
				 </br></br>

				Reading: 
				<ul>
					<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapter 20)
				</ul></br>
				
				Additional:
				<ul>
				<li> 
				</ul></br>



				<a name="lecture11"></a>
				<h2><span class="fa  fa-files-o"></span>  Lecture XX: Unsupervised learning: clustering </h2>
				Introduction to unsupervised learning methods. Data clustering. Hierarchical clustering. k-means clustering. Spectral clustering. </br></br>

				Reading: 
				<ul>
					<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapter 22)
				</ul></br>
				
				Additional:
				<ul>
				<li> U. von Luxburg. <a href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/luxburg06_TR_v2_4139%5b1%5d.pdf">Tutorial on spectral clustering</a>. Statistics and Computing 17(4), 2007
				</ul></br>
										

				<br><br>
				<div >
					<a name="structure"></a>
					<h2><span class="fa fa-file-text-o"></span> Course Structure and Objectives</h2>
					
					<font color="#10712C" size="4"> <b>Structure</b></font> </br>
					Each section of the course is divided into 1h30' lecture and 1h30' lab. The labs will include hands-on assignments (using Python) and will provide the students the opportunity to deal with ML tasks in practice. </br></br></br>	
					
					<font color="#10712C" size="4"> <b>Learning objectives</b></font> </br></br>
					The course aims to introduce students to the field of machine learning by:
					<ul>
					<li>Covering a wide range of topics, methodologies and related applications.
					<li>Giving the students the opportunity to obtain hands-on experience on dealing with.
					</ul>
					We expect that by the end of the course, the students will be able to:
					<ul>
					<li>Identify problems that can be solved using machine learning methodologies.
				 	<li> Given a problem, identify and apply the most appropriate algorithm(s).
				 	<li> Implement some of those algorithms from scratch.
				 	<li> Evaluate and compare machine learning algorithms for a
						particular task.
					<li> Deal with real-world data challenges.
					</ul>	
					</br></br></br>	

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Prerequisites</b></font> </br></br> 
					There is no official prerequisite for this course. However, the students are expected to: 
					<ul>
					<li> Have basic knowledge of probability theory and linear algebra.
					<li> Be familiar with at least one programming language (e.g., Python or any language of their preference).
					</ul>
					 </br></br></br>

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Reading material</b></font> </br></br> 
					There is no single requiered textbook for the course. We will recommend specific chapters from the following books:
					<ul>
					<li> Shai Shalev-Shwartz and Shai Ben-David. <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf"> Understanding Machine Learning: From Theory to Algorithms</a>. Cambridge University Press, 2014.
					<li> Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2011.
					<li> Trevor Hastie, Robert Tibshirani, and Jerome Friedman. <a href=" https://web.stanford.edu/~hastie/ElemStatLearn">The Elements of Statistical Learning: Data Mining, Inference, and Prediction. </a> Second Edition, Springer, 2017.
					<li> Jure Leskovec, Anand Rajaraman, and Jeff Ullman. <a href="http://infolab.stanford.edu/~ullman/mmds/ch11.pdf">Mining of Massive Datasets.</a> Cambridge University Press, 2014.	

					</ul> </br></br>

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Evaluation</b></font> </br></br> 
					The evaluation of the course will be based on the following:
					<ol> 
					<li> <b>Two assignments:</b> the assignments will include theoretical questions as well hands-on practical questions that will familiarize the students with basic machine learning tasks.
					<li> <b>Project:</b> The students are expected to form groups of 3-4 people, propose a topic for their project, and submit a final project report. Please, read the <a href="#project">project</a> section for more details.
					<li> <b> Final exam:</b> Final exam in the material covered in the course.	
					</ol> </br>

					The grading will be as follows: </br></br>
					<table  width=500>
					<col width="25%" />
					<col width="20%" />

						<tbody>
							<tr>
								<td> <b>Assignment 1</b> (individually):</td> <td> 10%</td>
							</tr>
							<tr>
								<td> <b>Assignment 2</b> (groups of 3-4 students):</td> <td> 20%</td>
							</tr>
							<tr>
							<td> <b>Project</b> (groups of 3-4 students):</td> <td> 35%</td>
							</tr>
							<tr>
							<td> <b>Final exam</b>:</td> <td> 35%</td>
							</tr>

					</table>
				</br></br>

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Academic integrity</b></font> </br></br> 
					All of your work must be your own. Don't copy another student's assignment, in part or in total, and submit it as your own work. Acknowledge and cite source material in your papers or assignments.
					</ul> </br></br>

				</div>



				<br><br>
				<div >
					<a name="project"></a>
					<h2><span class="fa fa-cogs"></span> Project</h2>
					Details about the project of the course have been posted  on piazza.
				</div> </br></br>




				<br><br>
				<div >
					<a name="resources"></a>
					<h2><span class="fa fa-external-link"></span> Resources</h2>

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Datasets</b></font> </br>

					<ul>
					<li> <a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#cite_note-129">List</a>  of datasets for machine learning reserach (mainly link to the corresponding research articles).
					<li> <a href="https://www.kaggle.com/datasets"> List</a>	of Kaggle datasets.	
					<li> <a href="http://www.kdnuggets.com/datasets/index.html"> KDnuggets </a> list of datasets.
					<li> <a href="https://archive.org/details/datasets"> archive.org </a> list of datasets.
					<li><a href="https://opendata.paris.fr/page/home/"> Paris Data</a>: publicly available datasets from the city of Paris.
					<li> <a href="https://kddcup2015.com/competition/kdd_2018/">KDD Cup of Fresh air</a>: dataset containing the concentration level or air pollutants (including PM2.5) in Beijing and London.
					<li><a href="https://www.crowdflower.com/data-for-everyone/"> CrowdFlower AI </a> list of datasets.
					<li> <a href="https://aws.amazon.com/datasets/"> Amazon Web Services </a> (AWS) list of datasets.	
					<li> <a href="http://jmcauley.ucsd.edu/data/amazon/"> Amazon product data </a> by Julian McAuley (UC San Diego).
					<li> <a href="http://jmcauley.ucsd.edu/data/amazon/qa/"> Amazon question/answer data </a> by Julian McAuley (UC San Diego).
					<li> <a href="http://snap.stanford.edu/">Stanford Network Analysis Project (SNAP)</a>.
					<li> <a href="http://socialcomputing.asu.edu/pages/home">Social Computing Data Repository at Arizona State University</a>.
					<li> <a href="https://github.com/BuzzFeedNews/everything"> BuzzFeedNews </a> datasets.
					<li> <a href="https://github.com/caesar0301/awesome-public-datasets"> Awesome public datasets</a>.
					<li> <a href="http://socialnetworks.mpi-sws.org/datasets.html">Datasets </a> from the Social Computing Research group at MPI-SWS.
					<li> <a href="https://aminer.org/data">Datasets </a> from the AMiner academic social network.
					<li> <a href="http://www.cs.ucr.edu/~eamonn/time_series_data/"> UCR Time Series Archive </a>	
					<li> <a href="http://www.icwsm.org/2016/datasets/datasets/">Datasets </a> from papers published in the  International AAAI Conference on Web and Social Media (ICWSM). Also, check the previous ICWSM conferences.
					<li> <a href="http://archive.ics.uci.edu/ml/index.php">UCI </a> of Machine Learning Repository.
					<li> <a href="http://www.emilio.ferrara.name/datasets/">Social media (Instagram and Facebook) </a> datasets by Emilio Ferrara (USC).
					<li> <a href="http://www.kdd.org/kdd-cup">KDD cup archives</a>, uncluding a competition about Tencent Weibo (KDD is one of the premier data mining conferences).
					<li> <a href="http://147.8.142.179/datazip/">Sina Weibo dataset </a> by Weiboscope (University of Hong Kong). Another Sina Weibo dataset by Tianchi can be found <a href="https://tianchi.aliyun.com/competition/information.htm?spm=5176.100067.5678.2.Vggj52&raceId=5">here</a>.
					<li> <a href="https://www.microsoft.com/en-us/research/publication/t-drive-trajectory-data-sample/">T-Drive trajectory data</a> by Microsof Research (one-week trajectories of 10,357 taxis).
					<li> <a href="https://sites.google.com/site/yangdingqi/home/foursquare-dataset">Foursquare dataset </a> by Dingqi Yang (University of Fribourg).
					<li> <a href="http://www.quanturb.com/data.html">Datasets </a> from the Spatial and Urban Networks group at CEA (Paris).	
					<li> <a href="https://www.yelp.com/dataset_challenge">Yelp dataset challenge</a>
					<li> <a href="https://www.kaggle.com/c/quora-question-pairs">Quora question pairs competition</a> in Kaggle (currently active).
					<li> <a href="http://illimine.cs.uiuc.edu/">IlliMine repository</a> from the Data Mining Research Group at UIUC.	

						
					</ul> </br>

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Software tools</b></font> </br>

					<ul>
					<li> <a href="http://scikit-learn.org/stable/">scikit-learn</a>: Machine Learning library in Python
					<li> <a href="http://pandas.pydata.org/">pandas</a>: Python data analysis library
					<li> <a href="http://seaborn.pydata.org/">seaborn</a>: statistical data visualization based on <a href="http://matplotlib.org/">matplotlib</a>
					<li> <a href="https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks">Gallery</a> of interesting Jupyter Notebooks	
					</ul> </br>



					<p class='bd'>
					<font color="#10712C" size="4"> <b>Related conferences</b></font> </br>
					Please find below a list of conferences related to the contents of the course (mostly in the field of machine learning and data mining. We provide the DBLP website of each venue where you can access the proceedings (papers, tutorials, etc). 

					</br> 
					<ul>
					<li><a href="http://dblp.uni-trier.de/db/conf/nips/">Neural Information Processing Systems (NIPS)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/icml/">International Conference on Machine Learning (ICML)</a>	
					<li><a href="http://dblp.uni-trier.de/db/conf/kdd/">Knowledge Discovery and Data Mining (KDD)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/icdm/index.html">IEEE International Conference on Data Mining (ICDM)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/sdm/">SIAM International Conference on Data Mining (SDM)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/www/index.html">International World Wide Web Conference (WWW)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/wsdm/index.html">Web Search and Data Mining (WSDM)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/cikm/">Information and Knowledge Management (CIKM)</a>	
					<li><a href="http://dblp.uni-trier.de/db/conf/pkdd/index.html">Machine Learning and Knowledge Discovery in Databases (ECML-PKDD)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/icwsm/index.html">International Conference on Web and Social Media (ICWSM)</a>
					</ul>
					Check out the website of each conference (e.g., <a href="http://www.kdd.org/kdd2016/">KDD 2016 </a>) for more information.
				</div>
			</div>	
		</div>
		<div class="navblank"></div>
		<div class="navblank"></div>
		<footer></footer>

		<script type="text/javascript" src="./static/js/jquery-1.10.2.min.js"></script>
		<script type="text/javascript" src="./static/js/bootstrap.min.js"></script>
		
	</body>
</html>