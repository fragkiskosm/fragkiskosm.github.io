<!DOCTYPE html>
<html lang="en">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
		<title>2EL1730: Machine Learning</title>
		<meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link href="./static/css/bootstrap.min.css" rel="stylesheet">
		<link href="./static/css/style.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
	</head>

	<body data-pinterest-extension-installed="cr1.40">
		<a name="home"></a>
		<nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
		  	<div class="container">
			    <!-- Brand and toggle get grouped for better mobile display -->
			    <div class="navbar-header">
					<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				        <span class="sr-only">Toggle navigation</span>
				        <span class="icon-bar"></span>
				        <span class="icon-bar"></span>
				        <span class="icon-bar"></span>
			      	</button>
			      	<a class="navbar-brand" href="#home"><strong>2EL1730 - F21/W22</strong></a>
			    </div>

			    <!-- Collect the nav links, forms, and other content for toggling -->
			    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			      	<ul class="nav navbar-nav navbar-right">
				        <li><a href="#overview"><span class="fa fa-bookmark-o"></span> Overview</a></li>
						<li><a href="#lectures"><span class="fa fa-calendar-check-o"></span> Schedule and Lectures</a></li>
						<li><a href="#structure"><span class="fa fa-file-text-o"></span>  Course Structure</a></li>
						<li><a href="#project"><span class="fa fa-cogs"></span> Project</a></li>
						<li><a href="#resources"><span class="fa fa-external-link"></span> Resources</a></li>
			      	</ul>
			    </div><!-- /.navbar-collapse -->
		  </div><!-- /.container-fluid -->
		</nav>


		<div class="navblank">
		</div>

		<div class="title_box jumbotron">
			<div class="container">
				<h1>2EL1730: Machine Learning</h1>			
				<div class="description"></div>
			</div>
		</div>

		<div class="container">

			<div class="content_box">
				<a name="overview"></a>
				<h2><span class="fa fa-bookmark-o"></span> Course Overview </h2>
				<blockquote class="text-justify"><p> 
				<b>General Info:</b> 2nd year course, CentraleSup&#233lec, November 2021 - January 2022, 35 hours </br></br>
				<b>Lecture hours:</b> Tuesday (8:30-11:45), Friday (13:45 - 17:00) </br>
				<b>Instructors:</b> <a href="http://fragkiskos.me">Fragkiskos Malliaros</a>  and <a href="http://cvn.centralesupelec.fr/~mariavak/">Maria Vakalopoulou</a> </br>
				<b>Online office hours:</b> Contact us on Teams
				</br></br>
				<b>TAs:</b> Stuti Jain, Lilly Monier</br>
				</br>

				<b>Edunao:</b> <a href="https://centralesupelec.edunao.com/course/view.php?id=4281">https://centralesupelec.edunao.com/course/view.php?id=4281</a> </br>

				</p>
				</blockquote>
				<br>
				
				<p align="justify">Machine learning  is the scientific field that provides computers the ability to learn without being explicitly programmed (definition by <a href="https://en.wikipedia.org/wiki/Machine_learning">Wikipedia</a>).  Machine learning lies at the heart of many real-world applications, including recommender systems, web search, computer vision, autonomous cars and automatic language translation.  </p>

				The course will provide an overview of fundamental topics as well as important trends in machine learning, including algorithms for supervised and unsupervised learning, dimensionality reduction methods and their applications. A substantial lab section will involve group projects on a data science competition and will provide the students the ability to apply the course theory to real-world problems. </p>
				<br>
				
				


				<br><br><br>
				<a name="lectures"></a>
				<h2><span class="fa fa-calendar-check-o"></span> Schedule and Lectures </h2>
				The topics of the lectures are subject to change (the following schedule outlines the topics that will be covered in the course). The slides for each lecture will be posted on <a href="https://centralesupelec.edunao.com/course/view.php?id=4281">Edunao</a> just before the start of the class. <b>The due dates of the assignments/project are subject to change.</b></br></br> 


					<div class="table_scrollable">
					<table class="table" width=100%>
					<col width="5%" />
					<col width="12%" />
					<col width="42%" />
					<col width="10%" />
					<col width="25%" />
						<thead>
							<tr>
								<th class="text-success">Lecture</th>
								<th class="text-success">Date</th>
								<th class="text-success">Topic</th>
								<th class="text-success">Material</th>
								<th class="text-success">Assignments/Project</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>1</td><td>November 30</td><td>Introduction; Model selection and evaluation</td><td><a href="#lecture1">Lecture 1</a></td><td></td>
							</tr>
							
							<tr>	
								<td>2</td><td>December 3</td><td>Linear and logistic regression</td><td><a href="#lecture2">Lecture 2</a></td><td></td>
							</tr>

							<tr>	
								<td>3</td><td>December 7</td><td>Probabilistic classifiers and linear discriminant analysis</td><td><a href="#lecture3">Lecture 3</a></td><td><b>Assignment 1 out</b></td>
							</tr>

							<tr>	
								<td>4</td><td>December 10</td><td>Non-parametric learning and nearest neighbor methods</td><td><a href="#lecture4">Lecture 4</a></td><td></td>
							</tr>
							
							<tr>	
								<td>5</td><td>December 17</td><td>Tree-based methods and ensemble learning</td><td><a href="#lecture5">Lecture 5</a></td><td><b>Assignment 1 due on December 21</b></td>
							</tr>

							<tr>	
								<td>6</td><td>January 4</td><td>Support Vector Machines</td><td><a href="#lecture6">Lecture 6</a></td><td><b>Kaggle project out</b></td>
							</tr>
					
							<tr>	
								<td>7</td><td>January 7</td><td>Neural networks</td><td><a href="#lecture7">Lecture 7</a></td><td><b>Assignment 2 out</b></td>
							</tr>
							<tr>	
								<td>8</td><td>January 11</td><td>Introduction to deep learning </br> </td><td><a href="#lecture8">Lecture 8</a></td><td></td>

							<tr>	
								<td>9</td><td>January 14</td><td>Dimensionality reduction</td><td><a href="#lecture9">Lecture 9</a></td><td></td>
							</tr>
								
							<tr>	
								<td>10</td><td>January 18</td><td>Unsupervised learning: clustering</td><td><a href="#lecture10">Lecture 10</a></td><td><b>Assignment 2 due on January 21</b></td>
							</tr>
							
							<tr>	
								<td>11</td><td>January 21</td><td>Topics in unsupervised learning</br><font size="2">Guest lecture</font> </td><td><a href="#lecture11">Lecture 11</a></td><td></td>
							</tr>

							<tr>	
								<td>12</td><td>January 24</td><td>Exams</td><td></td><td><b>Kaggle project due on January 30</b></td>
							</tr>	
						</tbody>
					</table>

				<!--
				<span id="emp"></span> <a href="http://fragkiskosm.github.io/papers/Tutorial_Slides_ICDM_2016.pdf"> Tutorial_Slides</a><br> <br> <br> --><br>
				
				</br>


				</br>
				<a name="lecture1"></a>
				<h2><span class="fa  fa-files-o"></span> [November 30] Lecture 1: Introduction; Model selection and evaluation</h2>
				Introduction to machine learning, administrivia, course structure and overview of the topics that will be covered in the course. Overfitting and generalization. Bias-variance tradeoff. Training, validation and test sets. Cross-validation. Evaluation of supervised learning algorithms. Basic concepts in optimization.  </br></br>

				Reading: 
				<ul>
					<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapters 1 and 2)
					<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn">The Elements of Statistical Learning</a> (Chapter 1)
					<li> <a href=" https://web.stanford.edu/~hastie/ElemStatLearn">The Elements of Statistical Learning: Data Mining, Inference, and Prediction </a> (Sections 7.1, 7.2, 7.3, 7.10)	
				</ul>
				</br>

				Additional:
				<ul>
					<li> M. Kuhn and K. Johnson. <a href="https://link.springer.com/chapter/10.1007/978-1-4614-6849-3_19/fulltext.html">An Introduction to Feature Selection</a>. Applied Predictive Modeling, pages 487-519, 2013. <i>[For the part of the lecture on feature selection].</i>
					<li> <a href="http://papail.io/teaching/901/lecture02_notes.pdf"> Concentration of the empirical risk</a> (also <a href="http://papail.io/teaching/901/scribe_02.pdf">here</a>), lecture notes by Dimitris Papailiopoulos (UW-Madison).	
					<li> Model evaluation, model selection, and algorithm selection in machine learning: <a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html">Part I</a> (Basics), <a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html">Part II</a> (Bootstrapping and uncertainties), and <a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html">Part III</a> (Cross-validation and hyperparameter tuning). Interesting blog post by <a href="https://sebastianraschka.com/">Sebastian Raschka</a>, 2016.
					<li> <a href="http://sbubeck.com/Bubeck15.pdf">Convex Optimization: Algorithms and Complexity</a> (Sections 1.1, 1.2, 1.3)
					<li> <a href="http://theory.epfl.ch/vishnoi/Nisheeth-VishnoiFall2014-ConvexOptimization.pdf">Convex optimization and gradient descent</a>, lecture notes by Nisheeth Vishnoi, EPFL (Sections 1.1, 1.2, 1.3)
				</ul></br>


				<a name="lecture2"></a>
				<h2><span class="fa  fa-files-o"></span>  [December 3] Lecture 2: Linear and logistic regression </h2>
				Supervised learning models. Linear regression. Regularization. Linear classification models. Logistic regression. Maximum likelihood estimation. </br></br>

				Reading: 
				<ul>
 				<li> <a href="https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf">Linear and logistic regression</a>, lecture notes by Andrew Ng (Stanford University)
 				<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn">The Elements of Statistical Learning</a> (Sections 3.1, 3.2 and 3.4)
				</ul>

				Additional:
				<ul>
				<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Sections 9.2 and 9.3)
				</ul></br>

				
				<a name="lecture3"></a>
				<h2><span class="fa  fa-files-o"></span>  [December 7] Lecture 3: Probabilistic classifiers and linear discriminant analysis </h2>
				Bayes rule. Naive Bayes classifier. Maximum a posteriori estimation. Linear discriminant analysis (LDA). </br></br>

				Reading:
				<ul> 
				<li> <a href="https://see.stanford.edu/materials/aimlcs229/cs229-notes2.pdf">Generative learning algorithms</a>, lecture notes by Andrew Ng (Stanford University)
				<li> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a> (Chapter 4.1)	
				<li> <a href="https://www.ccis.northeastern.edu/home/vip/teach/MLcourse/5_features_dimensions/lecture_notes/LDA/LDA.pdf">Fisher linear discriminant analysis (LDA)</a>, lecture notes by Cheng Li and Bingyu Wang (Northeastern University)	
				<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn">The Elements of Statistical Learning</a> (Section 4.3)
				</ul>

				Additional:
				<ul>
				<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Sections 24.1, 24.2 and 24.3)
				<li> <a href="https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Machine%20Learning_%20A%20Probabilistic%20Perspective%20%5BMurphy%202012-08-24%5D.pdf">Machine Learning - A Probabilistic Perspective</a> (Chapter 4)		
				<li> <a href="https://www.doc.ic.ac.uk/~dfg/ProbabilisticInference/old_IDAPILecture15.pdf">Linear discriminant analysis (LDA)</a>, lecture notes by Duncan Fyfe Gillies (Imperial College)
				<li> <a href="https://www.ics.uci.edu/~welling/teaching/273ASpring09/Fisher-LDA.pdf">Fisher linear discriminant analysis</a>, notes by Max Welling (University of Amsterdam)	
				</ul></br>




				<a name="lecture4"></a>
				<h2><span class="fa  fa-files-o"></span> [December 10] Lecture 4: Non-parametric learning and nearest neighbor methods </h2>
				Introduction to non-parametric learning methods. Distance and similarity metrics. Nearest neighbor algorithms. </br></br>

				Reading: 
				<ul>
				<li> <a href="http://ciml.info/dl/v0_99/ciml-v0_99-ch03.pdf">	A Course in Machine Learning</a>, by Hal Daumé III (Sections 3.1, 3.2 and 3.3)
				</ul>

				Additional:
				<ul>
				<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapter 19)	
				<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn">The Elements of Statistical Learning</a> (Section 13.3)
				</ul></br>

				


				<a name="lecture5"></a>
				<h2><span class="fa  fa-files-o"></span> [December 17] Lecture 5: Tree-based methods and ensemble learning</h2>
				Decision trees. Ensemble learning. Bagging and Boosting. The AdaBoost algorithm.</br></br>

				Reading:
				<ul> 
				<li> <a href="https://www-users.cs.umn.edu/~kumar001/dmbook/ch4.pdf">Classification: Basic Concepts, Decision Trees, and Model Evaluation</a>, Introduction to Data Mining, by Pang-Ning Tan, Michael Steinbach, and Vipin Kumar, 2006
				<li> <a href="https://www.lri.fr/~kegl/mlcourse/book.pdf">Introduction to AdaBoost</a>, lecture notes by Balazs Kegl (University of Paris-Saclay)
				</ul>

				Additional:
				<ul>
				<li><a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapters 18 and 10) 
				</ul></br>



				<a name="lecture6"></a>
				<h2><span class="fa  fa-files-o"></span> [January 4] Lecture 6: Support Vector Machines</h2>
				Maximum margin classifier. Linear SVMs. Primal and dual optimization problems. Non-linearly separable data and the kernel trick. Regularization and the non-separable case. </br></br>
				

				Reading: 
				<ul>
					<li> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a> (Chapter 7.1)
					<li> <a href="https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf">Support Vector Machines</a>, lecture notes by Andrew Ng (Stanford University)
				</ul> </br>

				Additional:
				<ul>
				<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapter 15)
				</ul></br>




				<a name="lecture7"></a>
				<h2><span class="fa  fa-files-o"></span>  [January 7] Lecture 7: Neural networks </h2>
				 Introduction to neural networks. The perceptron algorithm. Multilayer perceptron. Backpropagation. Applications.  
				</br></br>

				Reading: 
				<ul>
					<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapter 20)
				</ul></br>


				<a name="lecture8"></a>
				<h2><span class="fa  fa-files-o"></span>  [January 11] Lecture 8: Introduction to deep learning </h2>
				 Deep learning, CNNs 
				</br></br>

				Reading: 
				<ul>
					<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapter 20)
				</ul></br>



				<a name="lecture9"></a>
				<h2><span class="fa  fa-files-o"></span> [January 14] Lecture 9: Dimensionality reduction</h2>
				Dimemensionality reduction techniques. Singular Value Decomposition (SVD). Principal Component Analysis (PCA). Multidimensional Scaling (MDS) and nonlinear dimensionality reduction.
				</br></br>

				Reading: 
				<ul>
					<li> <a href="http://infolab.stanford.edu/~ullman/mmds/ch11.pdf">Mining of Massive Datasets</a> (Sections 11.1, 11.2, 11.3)
					<li> Jonathon Shlens. <a href="https://arxiv.org/pdf/1404.1100.pdf">A Tutorial on Principal Component Analysis</a>. arXiv, 2014.
				</ul></br>

				Additional:
				<ul>
					<li> <a href="http://theory.stanford.edu/~tim/s15/l/l9.pdf"> SVD and Low Rank Matrix Approximations</a>, lecture notes by Tim Roughgarden and Gregory Valiant (Stanford University)
					<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf"> Understanding Machine Learning: From Theory to Algorithms</a> (Section 23.1)
					<li>J. B. Tenenbaum, V. De Silva, and J. C. Langford. <a href="http://wearables.cc.gatech.edu/paper_of_week/isomap.pdf">A Global Geometric Framework for Nonlinear Dimensionality Reduction</a>. Science, 290:5500, pp. 2319-2323, 2000
					<li> S. T. Roweis and L. K. Saul. <a href="http://www.robots.ox.ac.uk/~az/lectures/ml/lle.pdf">Nonlinear Dimensionality Reduction by Locally Linear Embedding</a>. Science, 290:5500, pp. 2323-2326, 2000
					<li> M. Belkin and P. Niyogi. <a href="https://www.cise.ufl.edu/class/cap6617fa15/Readings/LEM_NIPS_01.pdf">Laplacian eigenmaps and spectral techniques for embedding and clustering</a>. In NIPS, 2001	
				</ul></br>
			


				<a name="lecture10"></a>
				<h2><span class="fa  fa-files-o"></span>  [January 18] Lecture 10: Unsupervised learning: clustering </h2>
				Introduction to unsupervised learning methods. Data clustering. Hierarchical clustering. k-means clustering. Spectral clustering. </br></br>

				Reading: 
				<ul>
					<li> <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms</a> (Chapter 22)
				</ul></br>
				
				Additional:
				<ul>
				<li> U. von Luxburg. <a href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/luxburg06_TR_v2_4139%5b1%5d.pdf">Tutorial on spectral clustering</a>. Statistics and Computing 17(4), 2007
				</ul></br>


				<a name="lecture11"></a>
				<h2><span class="fa  fa-files-o"></span>  [January 21] Lecture 11: Introduction to reinforcement learning </h2>
				Intelligence agents, dynamic programming, Monte Carlo methods, temporal difference learning
				 </br></br>

				Reading: 
				<ul>
					<li> <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Reinforcement Learning: An Introduction</a>
				</ul></br>
										

				<br><br>
				<div >
					<a name="structure"></a>
					<h2><span class="fa fa-file-text-o"></span> Course Structure and Objectives</h2>
					
					<font color="#10712C" size="4"> <b>Structure</b></font> </br>
					Each section of the course is divided into 1h30' lecture and 1h30' lab. The labs will include hands-on assignments (using Python) and will provide the students the opportunity to deal with ML tasks in practice. </br></br></br>	
					
					<font color="#10712C" size="4"> <b>Learning objectives</b></font> </br></br>
					The course aims to introduce students to the field of machine learning by:
					<ul>
					<li>Covering a wide range of topics, methodologies and related applications.
					<li>Giving the students the opportunity to obtain hands-on experience on dealing with.
					</ul>
					We expect that by the end of the course, the students will be able to:
					<ul>
					<li>Identify problems that can be solved using machine learning methodologies.
				 	<li> Given a problem, identify and apply the most appropriate algorithm(s).
				 	<li> Implement some of those algorithms from scratch.
				 	<li> Evaluate and compare machine learning algorithms for a
						particular task.
					<li> Deal with real-world data challenges.
					</ul>	
					</br></br></br>	

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Prerequisites</b></font> </br></br> 
					There is no official prerequisite for this course. However, the students are expected to: 
					<ul>
					<li> Have basic knowledge of probability theory and linear algebra.
					<li> Be familiar with at least one programming language (e.g., Python or any language of their preference).
					</ul>
					 </br></br></br>

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Reading material</b></font> </br></br> 
					There is no single requiered textbook for the course. We will recommend specific chapters from the following books:
					<ul>
					<li> Shai Shalev-Shwartz and Shai Ben-David. <a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf"> Understanding Machine Learning: From Theory to Algorithms</a>. Cambridge University Press, 2014.
					<li> Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2011.
					<li> Trevor Hastie, Robert Tibshirani, and Jerome Friedman. <a href=" https://web.stanford.edu/~hastie/ElemStatLearn">The Elements of Statistical Learning: Data Mining, Inference, and Prediction. </a> Second Edition, Springer, 2017.
					<li> Jure Leskovec, Anand Rajaraman, and Jeff Ullman. <a href="http://infolab.stanford.edu/~ullman/mmds/ch11.pdf">Mining of Massive Datasets.</a> Cambridge University Press, 2014.	

					</ul> </br></br>

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Evaluation</b></font> </br></br> 
					The evaluation of the course will be based on the following:
					<ol> 
					<li> <b>Two assignments:</b> the assignments will include theoretical questions as well hands-on practical questions that will familiarize the students with basic machine learning tasks.
					<li> <b>Project:</b> this will be a Kaggle challenge. The students are expected to form groups of 3-4 people, work on the challenge, and submit a final report.
					<li> <b> Final exam:</b> Final exam in the material covered in the course.	
					</ol> </br>

					The grading will be as follows: </br></br>
					<table  width=500>
					<col width="30%" />
					<col width="20%" />

						<tbody>
							<tr>
								<td> <b>Assignment 1</b> (individually):</td> <td> 10%</td>
							</tr>
							<tr>
								<td> <b>Assignment 2</b> (individually):</td> <td> 10%</td>
							</tr>
							<tr>
							<td> <b>Kaggle project</b> (groups of 3-4 students):</td> <td> 20%</td>
							</tr>
							<tr>
							<td> <b>Final exam</b>:</td> <td> 60%</td>
							</tr>

					</table>
				</br></br>

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Academic integrity</b></font> </br></br> 
					All of your work must be your own. Don't copy another student's assignment, in part or in total, and submit it as your own work. Acknowledge and cite source material in your papers or assignments.
					</ul> </br></br>

				</div>



				<br><br>
				<div >
					<a name="project"></a>
					<h2><span class="fa fa-cogs"></span> Project</h2>
					Details about the project of the course have been posted  on Edunao.
				</div> </br></br>




				<br><br>
				<div >
					<a name="resources"></a>
					<h2><span class="fa fa-external-link"></span> Resources</h2>

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Datasets</b></font> </br>

					<ul>
					<li> <a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#cite_note-129">List</a>  of datasets for machine learning reserach (mainly link to the corresponding research articles).
					<li> <a href="https://www.kaggle.com/datasets"> List</a>	of Kaggle datasets.	
					<li> <a href="http://www.kdnuggets.com/datasets/index.html"> KDnuggets </a> list of datasets.
					<li> <a href="https://archive.org/details/datasets"> archive.org </a> list of datasets.
					<li><a href="https://opendata.paris.fr/page/home/"> Paris Data</a>: publicly available datasets from the city of Paris.
					<li> <a href="https://kddcup2015.com/competition/kdd_2018/">KDD Cup of Fresh air</a>: dataset containing the concentration level or air pollutants (including PM2.5) in Beijing and London.
					<li><a href="https://www.crowdflower.com/data-for-everyone/"> CrowdFlower AI </a> list of datasets.
					<li> <a href="https://aws.amazon.com/datasets/"> Amazon Web Services </a> (AWS) list of datasets.	
					<li> <a href="http://jmcauley.ucsd.edu/data/amazon/"> Amazon product data </a> by Julian McAuley (UC San Diego).
					<li> <a href="http://jmcauley.ucsd.edu/data/amazon/qa/"> Amazon question/answer data </a> by Julian McAuley (UC San Diego).
					<li> <a href="http://snap.stanford.edu/">Stanford Network Analysis Project (SNAP)</a>.
					<li> <a href="http://socialcomputing.asu.edu/pages/home">Social Computing Data Repository at Arizona State University</a>.
					<li> <a href="https://github.com/BuzzFeedNews/everything"> BuzzFeedNews </a> datasets.
					<li> <a href="https://github.com/caesar0301/awesome-public-datasets"> Awesome public datasets</a>.
					<li> <a href="http://socialnetworks.mpi-sws.org/datasets.html">Datasets </a> from the Social Computing Research group at MPI-SWS.
					<li> <a href="https://aminer.org/data">Datasets </a> from the AMiner academic social network.
					<li> <a href="http://www.cs.ucr.edu/~eamonn/time_series_data/"> UCR Time Series Archive </a>	
					<li> <a href="http://www.icwsm.org/2016/datasets/datasets/">Datasets </a> from papers published in the  International AAAI Conference on Web and Social Media (ICWSM). Also, check the previous ICWSM conferences.
					<li> <a href="http://archive.ics.uci.edu/ml/index.php">UCI </a> of Machine Learning Repository.
					<li> <a href="http://www.emilio.ferrara.name/datasets/">Social media (Instagram and Facebook) </a> datasets by Emilio Ferrara (USC).
					<li> <a href="http://www.kdd.org/kdd-cup">KDD cup archives</a>, uncluding a competition about Tencent Weibo (KDD is one of the premier data mining conferences).
					<li> <a href="http://147.8.142.179/datazip/">Sina Weibo dataset </a> by Weiboscope (University of Hong Kong). Another Sina Weibo dataset by Tianchi can be found <a href="https://tianchi.aliyun.com/competition/information.htm?spm=5176.100067.5678.2.Vggj52&raceId=5">here</a>.
					<li> <a href="https://www.microsoft.com/en-us/research/publication/t-drive-trajectory-data-sample/">T-Drive trajectory data</a> by Microsof Research (one-week trajectories of 10,357 taxis).
					<li> <a href="https://sites.google.com/site/yangdingqi/home/foursquare-dataset">Foursquare dataset </a> by Dingqi Yang (University of Fribourg).
					<li> <a href="http://www.quanturb.com/data.html">Datasets </a> from the Spatial and Urban Networks group at CEA (Paris).	
					<li> <a href="https://www.yelp.com/dataset_challenge">Yelp dataset challenge</a>
					<li> <a href="https://www.kaggle.com/c/quora-question-pairs">Quora question pairs competition</a> in Kaggle (currently active).
					<li> <a href="http://illimine.cs.uiuc.edu/">IlliMine repository</a> from the Data Mining Research Group at UIUC.	

						
					</ul> </br>

					<p class='bd'>
					<font color="#10712C" size="4"> <b>Software tools</b></font> </br>

					<ul>
					<li> <a href="http://scikit-learn.org/stable/">scikit-learn</a>: Machine Learning library in Python
					<li> <a href="http://pandas.pydata.org/">pandas</a>: Python data analysis library
					<li> <a href="http://seaborn.pydata.org/">seaborn</a>: statistical data visualization based on <a href="http://matplotlib.org/">matplotlib</a>
					<li> <a href="https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks">Gallery</a> of interesting Jupyter Notebooks	
					</ul> </br>



					<p class='bd'>
					<font color="#10712C" size="4"> <b>Related conferences</b></font> </br>
					Please find below a list of conferences related to the contents of the course (mostly in the field of machine learning and data mining. We provide the DBLP website of each venue where you can access the proceedings (papers, tutorials, etc). 

					</br> 
					<ul>
					<li><a href="http://dblp.uni-trier.de/db/conf/nips/">Neural Information Processing Systems (NIPS)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/icml/">International Conference on Machine Learning (ICML)</a>	
					<li><a href="http://dblp.uni-trier.de/db/conf/kdd/">Knowledge Discovery and Data Mining (KDD)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/icdm/index.html">IEEE International Conference on Data Mining (ICDM)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/sdm/">SIAM International Conference on Data Mining (SDM)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/www/index.html">International World Wide Web Conference (WWW)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/wsdm/index.html">Web Search and Data Mining (WSDM)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/cikm/">Information and Knowledge Management (CIKM)</a>	
					<li><a href="http://dblp.uni-trier.de/db/conf/pkdd/index.html">Machine Learning and Knowledge Discovery in Databases (ECML-PKDD)</a>
					<li><a href="http://dblp.uni-trier.de/db/conf/icwsm/index.html">International Conference on Web and Social Media (ICWSM)</a>
					</ul>
					Check out the website of each conference (e.g., <a href="https://www.kdd.org/kdd2020/">KDD 2020 </a>) for more information.
				</div>
			</div>	
		</div>
		<div class="navblank"></div>
		<div class="navblank"></div>
		<footer></footer>

		<script type="text/javascript" src="./static/js/jquery-1.10.2.min.js"></script>
		<script type="text/javascript" src="./static/js/bootstrap.min.js"></script>
		
	</body>
</html>